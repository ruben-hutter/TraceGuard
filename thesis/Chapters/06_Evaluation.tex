% !TEX root = ../Thesis.tex
\chapter{Evaluation}

This chapter presents a comprehensive empirical evaluation of TraceGuard's taint-guided symbolic execution approach. The evaluation compares TraceGuard against classical symbolic execution techniques using a systematic benchmarking methodology across diverse test scenarios. The results demonstrate that TraceGuard achieves better vulnerability detection capabilities, particularly in challenging scenarios where classical approaches struggle, while maintaining competitive exploration efficiency through focused path prioritization.

\section{Experimental Design}

\subsection{Research Questions}\label{subsec:research_questions}

The evaluation addresses four primary research questions regarding the effectiveness and efficiency of taint-guided symbolic execution:

\begin{enumerate}
    \item \textbf{RQ1:} How does taint-guided exploration compare to classical symbolic execution in terms of vulnerability discovery rate and detection reliability?
    \item \textbf{RQ2:} What is the computational overhead of taint tracking and scoring mechanisms compared to standard exploration strategies?
    \item \textbf{RQ3:} How effectively does the approach control state explosion while maintaining comprehensive security analysis?
    \item \textbf{RQ4:} What is the scalability of taint-guided exploration across programs with varying complexity and control flow patterns?
\end{enumerate}

\subsection{Evaluation Metrics}

The evaluation employs multiple quantitative metrics to comprehensively assess TraceGuard's performance characteristics:

\begin{itemize}
    \item \textbf{Effectiveness Metrics:} Vulnerability detection rate, number of vulnerabilities found, detection consistency across multiple runs
    \item \textbf{Efficiency Metrics:} Total execution time
    \item \textbf{Coverage Metrics:} Basic block coverage, path exploration efficiency, coverage-to-vulnerability ratio
    \item \textbf{Scalability Metrics:} Performance under state explosion conditions, behavior with increasing program complexity
    \item \textbf{Reliability Metrics:} Success rate across multiple executions, statistical variance in performance measurements
\end{itemize}

\subsection{Experimental Environment}

All experiments were conducted on a standardized environment to ensure reproducible results. The benchmarking system executed each test scenario 10 times with a 120-second timeout per execution to capture performance variance and establish statistical significance. Both TraceGuard and classical angr symbolic execution were configured with identical resource constraints and analysis parameters.

\section{Benchmark Programs}

\subsection{Synthetic Test Suite}

The evaluation employs a carefully designed synthetic benchmark suite consisting of seven distinct test programs, each targeting specific aspects of symbolic execution performance:

\begin{itemize}
    \item \textbf{simple\_test:} Basic vulnerability detection with minimal control flow complexity
    \item \textbf{test\_conditional\_explosion:} Programs with numerous conditional branches to evaluate path exploration efficiency
    \item \textbf{test\_deep\_exploration:} Deep call stack scenarios testing exploration depth handling
    \item \textbf{test\_many\_functions:} Multi-function programs evaluating inter-procedural analysis performance
    \item \textbf{test\_perfect\_scenario:} Ideal taint flow patterns to evaluate TraceGuard's performance under optimal conditions
    \item \textbf{test\_recursive\_exploration:} Recursive function calls testing loop and recursion handling
    \item \textbf{test\_state\_explosion:} Complex control flow designed to trigger state explosion conditions
\end{itemize}

Each test program contains known vulnerabilities with well-defined taint flow patterns, enabling precise measurement of detection effectiveness and analysis efficiency.

\section{Experimental Results}

\subsection{Vulnerability Detection Effectiveness}

\textbf{Perfect Detection Rate:} TraceGuard demonstrates exceptional vulnerability detection reliability, achieving a 100\% detection rate across all test scenarios and execution runs. This result directly addresses RQ1 (Section~\ref{subsec:research_questions}), confirming that taint-guided exploration maintains detection effectiveness while optimizing exploration strategy.

Table~\ref{tab:detection_results} summarizes the vulnerability detection performance. TraceGuard consistently identified all embedded vulnerabilities across 70 total execution runs (7 programs Ã— 10 runs each), demonstrating robust detection reliability and, in the most challenging scenario, superior performance compared to classical symbolic execution.

\begin{table}[htbp]
\centering
\caption{Vulnerability Detection Summary}
\label{tab:detection_results}
\begin{tabular}{lccc}
\toprule
\textbf{Test Program} & \textbf{TraceGuard} & \textbf{Classical} & \textbf{Detection Rate} \\
\midrule
simple\_test & 1.0 & 1.0 & 100\% \\
test\_conditional\_explosion & 1.0 & 1.0 & 100\% \\
test\_deep\_exploration & 1.0 & 1.0 & 100\% \\
test\_many\_functions & 1.0 & 1.0 & 100\% \\
test\_perfect\_scenario & 1.0 & 1.0 & 100\% \\
test\_recursive\_exploration & 1.0 & 1.0 & 100\% \\
test\_state\_explosion & 5.0 & 1.0 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Superior Performance in Challenging Scenarios:} The \texttt{test\_state\_explosion} scenario provides the most significant and compelling results. TraceGuard identified 5 vulnerabilities while classical symbolic execution identified only 1 vulnerability, demonstrating a $5 \times$ improvement in vulnerability discovery effectiveness. This represents a fundamental advantage where taint-guided exploration excels in scenarios designed to challenge symbolic execution systems through complex control flow patterns.

\subsection{Execution Time Performance}

\textbf{Competitive Execution Times with Notable Improvements:} TraceGuard demonstrates competitive execution performance across the benchmark suite, with improvements in several scenarios and acceptable overhead in others. Table~\ref{tab:execution_times} presents detailed timing analysis addressing RQ2 (Section~\ref{subsec:research_questions}) regarding computational overhead.

\begin{table}[htbp]
\centering
\caption{Execution Time Comparison (seconds)}
\label{tab:execution_times}
\begin{tabular}{lccr}
\toprule
\textbf{Test Program} & \textbf{TraceGuard} & \textbf{Classical} & \textbf{Improvement} \\
\midrule
simple\_test & 8.07 $\pm$ 1.15 & 8.06 $\pm$ 1.39 & -0.1\% \\
test\_conditional\_explosion & 7.91 $\pm$ 1.57 & 8.65 $\pm$ 2.06 & +8.5\% \\
test\_deep\_exploration & 9.12 $\pm$ 1.34 & 8.80 $\pm$ 1.18 & -3.6\% \\
test\_many\_functions & 5.69 $\pm$ 0.28 & 5.73 $\pm$ 0.33 & +0.8\% \\
test\_perfect\_scenario & 12.72 $\pm$ 1.88 & 12.22 $\pm$ 1.77 & -4.1\% \\
test\_recursive\_exploration & 9.83 $\pm$ 0.80 & 9.71 $\pm$ 0.75 & -1.2\% \\
test\_state\_explosion & 92.14 $\pm$ 13.94 & 87.48 $\pm$ 12.36 & -5.3\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Effective State Explosion Management:} The most significant finding occurs in the \texttt{test\_state\_explosion} scenario, where TraceGuard achieves comparable execution time (92.14s vs 87.48s, -5.3\%) while detecting 5 vulnerabilities compared to classical symbolic execution's single vulnerability detection. This result addresses RQ3 (Section~\ref{subsec:research_questions}) by demonstrating that taint-guided prioritization maintains reasonable analysis efficiency while dramatically improving vulnerability discovery effectiveness in the most challenging scenarios.

\textbf{Performance in Complex Branching:} The \texttt{test\_conditional\_explosion} scenario shows an 8.5\% execution time improvement, demonstrating that taint-guided exploration effectively navigates complex conditional logic without exhaustive path enumeration. The \texttt{test\_many\_functions} scenario also achieves a 0.8\% improvement, indicating effective handling of inter-procedural analysis.

\subsection{Coverage Analysis and Path Efficiency}

\textbf{Focused Exploration Strategy:} TraceGuard consistently achieves significantly reduced basic block coverage compared to classical symbolic execution, ranging from 36.8\% to 75.0\% of classical coverage across different test scenarios. This reduction represents the core advantage of taint-guided exploration: achieving superior security analysis results through focused path selection.

Table~\ref{tab:coverage_efficiency} illustrates the coverage efficiency across all test programs. The consistent pattern of reduced coverage with maintained or superior vulnerability detection demonstrates that TraceGuard successfully identifies and prioritizes security-relevant execution paths while avoiding exhaustive exploration of security-irrelevant code regions.

\begin{table}[htbp]
\centering
\caption{Coverage Efficiency Analysis}
\label{tab:coverage_efficiency}
\begin{tabular}{lccc}
\toprule
\textbf{Test Program} & \textbf{TraceGuard} & \textbf{Classical} & \textbf{Efficiency Ratio} \\
\midrule
simple\_test & 9.0 $\pm$ 0.0 & 12.0 $\pm$ 0.0 & 75.0\% \\
test\_conditional\_explosion & 10.0 $\pm$ 0.0 & 17.0 $\pm$ 0.0 & 58.8\% \\
test\_deep\_exploration & 13.0 $\pm$ 0.0 & 24.0 $\pm$ 0.0 & 54.2\% \\
test\_many\_functions & 12.0 $\pm$ 0.0 & 17.0 $\pm$ 0.0 & 70.6\% \\
test\_perfect\_scenario & 11.0 $\pm$ 0.0 & 27.0 $\pm$ 0.0 & 40.7\% \\
test\_recursive\_exploration & 12.0 $\pm$ 0.0 & 21.0 $\pm$ 0.0 & 57.1\% \\
test\_state\_explosion & 13.0 $\pm$ 0.0 & 35.3 $\pm$ 0.5 & 36.8\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Quality over Quantity:} The coverage analysis reveals that TraceGuard's reduced exploration is not a limitation but rather a strategic advantage. In the most challenging scenario (\texttt{test\_state\_explosion}), TraceGuard explores only 36.8\% of the basic blocks covered by classical execution yet finds $5 \times$ more vulnerabilities, demonstrating the effectiveness of focused, security-oriented exploration.

\subsection{Statistical Significance and Reliability}

All performance measurements demonstrate high statistical reliability with acceptable variance across multiple execution runs. The 100\% success rate across all 70 execution runs confirms the stability and robustness of the TraceGuard implementation. Standard deviations in execution times remain within reasonable bounds, indicating consistent performance characteristics suitable for practical deployment.

\section{Discussion and Analysis}

The experimental results demonstrate that TraceGuard successfully addresses the core research questions, achieving superior vulnerability detection effectiveness while maintaining competitive computational performance. However, several limitations and practical considerations must be acknowledged for a complete assessment of the approach.

\subsection{Limitations and Practical Considerations} The evaluation relies on carefully crafted synthetic test programs with predefined taint sources (primarily \texttt{fgets} and similar input functions) and known vulnerability patterns. This controlled environment may not fully represent the complexity of real-world commercial software, where taint sources can be diverse, indirect, or context-dependent. The effectiveness of the approach on large-scale applications with complex data flow patterns remains to be validated.

\textbf{Taint Source Identification Dependency:} TraceGuard's effectiveness is fundamentally dependent on accurate identification of taint sources. The current implementation focuses on standard input functions, but real-world applications may receive untrusted data through network protocols, file formats, inter-process communication, or other vectors that require additional analysis to identify as taint sources.

\textbf{Limited Architecture and Language Scope:} The evaluation focuses exclusively on AMD64 binaries compiled from C/C++ programs. Generalization to other architectures, programming languages, or execution environments (such as interpreted languages or virtual machines) requires additional validation and potentially significant implementation modifications.

\textbf{Scalability Assumptions:} While the results suggest positive scalability characteristics, the evaluation does not include large-scale programs or complex commercial software systems. The overhead of taint tracking and scoring mechanisms may become more significant as program size and complexity increase, particularly in applications with extensive library dependencies or complex control flow structures.

\vspace{1em}

Despite these limitations, the evaluation demonstrates that the core contribution of taint-guided prioritization effectively addresses fundamental scalability challenges in symbolic execution, providing a foundation for future development of more comprehensive security analysis tools.
